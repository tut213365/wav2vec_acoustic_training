# @package _group_

common:
    fp16: true
    log_format: json
    log_interval: 200

checkpoint:
    save_interval_updates: 25000
    keep_interval_updates: 1
    no_epoch_checkpoints: true

task:
    _name: audio_pretraining
    data: ???
    max_sample_size: 400000
    min_sample_size: 16000
    normalize: false

dataset:
    num_workers: 12
    max_tokens: 1400000
    skip_invalid_size_inputs_valid_test: true

distributed_training:
    distributed_world_size: 64
    ddp_backend: legacy_ddp

criterion:
    _name: wav2vec
    infonce: true
    log_keys: ["prob_perplexity", "code_perplexity", "temp"]
    loss_weights: [0.1, 10]

optimization:
    max_update: 400000
    lr: [0.0005]

optimizer:
    _name: adam
    adam_betas: (0.9,0.98)
    adam_eps: 1e-06
    weight_decay: 0.01

lr_scheduler:
    _name: polynomial_decay
    warmup_updates: 32000

model:
    _name: w2v2_selftune
    w2v_path: ???